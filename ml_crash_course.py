# -*- coding: utf-8 -*-
"""ML Crash Course.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Fa23wCaYx7FNlGUbdplP1wbvwz4BRPCB

# Predict if a customer will “Churn” or not
"""

# Import pandas library
import pandas as pd

# Read in data from CSV
df = pd.read_csv('https://raw.githubusercontent.com/kapilbhise/ml-lab/main/WA_Fn-UseC_-Telco-Customer-Churn.csv')

# View first five rows of data
df.head()

"""Data Understanding"""

# View length of data frame
len(df)

# Check columns
df.columns

# Check datatypes
df.dtypes

# View unique values within each column
for col in df.columns:
     print(col, len(df[col].unique()), df[col].unique())

# Calculate summary statistics numerical variables
df.describe()

# Calculate summary statistics for categorical variables
df.describe(include=['object'])

# Get columns which have less than two variables
df['SeniorCitizen'].dtype =='object'
less_than_2 = []
for col in df.columns:
        if len(df[col].unique()) == 2 and df[col].dtype=='object':
            print(col)
            less_than_2.append(col)

# Calculate number of missing values
df.isnull().sum()

# Create a pairplot
import seaborn as sns
from matplotlib import pyplot as plt

plt.figure(figsize=(14,14))
sns.pairplot(df)
plt.show()

"""Data Preparation"""

# One hot encode columns with less than 2 unique values
less_than_2
for col in less_than_2:
    if col == 'gender':
        df[col] = df[col].apply(lambda x: 1 if x=='Female' else 0)
    else:
         df[col] = df[col].apply(lambda x: 1 if x=='Yes' else 0)

# Look for blank strings within Total Charges column
df[df['TotalCharges']==' '].head()

df['TotalCharges'].replace(' ', 0, inplace=True)

# Convert Total Charges column to float
df['TotalCharges'] = df['TotalCharges'].astype(float)

# Calculate correlation
df.corr()

# Plot heatmap from correlation
plt.figure(figsize=(9,9))
sns.heatmap(df.corr())
plt.show()

# Drop identifier column
df.drop('customerID', axis=1, inplace=True)

# Create one hot encoding for one column
pd.get_dummies(df['MultipleLines']).head()

# One hot encode an entire data frame and store it in a new dataframe called abt
abt = pd.get_dummies(df)

abt

# Create X and y variables
y = abt['TotalCharges']
X = abt.drop('TotalCharges', axis=1)

# Create training and testing splits
random_state=1234
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=random_state)

# Print out training and testing split lengths
print(len(X_train), len(X_test), len(y_train), len(y_test))

"""Data Modeling"""

# Import pipeline helpers
from sklearn.pipeline import make_pipeline

# Import scaling helpers
from sklearn.preprocessing import StandardScaler

# Import algorithms
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor

# Create pipeline dictionaries
pipelines = {
     'rf':make_pipeline(StandardScaler(), RandomForestRegressor(random_state=random_state)),
     'gb':make_pipeline(StandardScaler(), GradientBoostingRegressor(random_state=random_state)),
     'enet':make_pipeline(StandardScaler(), ElasticNet(random_state=random_state)),
     'ridge':make_pipeline(StandardScaler(), Ridge(random_state=random_state)),
     'lasso':make_pipeline(StandardScaler(), Lasso(random_state=random_state))
 }

# Create hyperparameter optimization grids
grid = {
     'rf':{'randomforestregressor__n_estimators':[10,20,30]},
     'gb':{'gradientboostingregressor__alpha':[0.5,0.9,0.99]},
     'enet':{'elasticnet__alpha':[0.5,0.9,0.99]},
     'ridge':{'ridge__alpha':[0.5,0.9,0.99]},
     'lasso':{'lasso__alpha':[0.5,0.9,0.99]}
 }

# View tunable parameters
Lasso().get_params()

from sklearn.model_selection import GridSearchCV

# Train models and save them to fit_models dictionary
fit_models = {}
for algo, pipeline in pipelines.items():
    # 4.1 Create a Grid Search CV instance
    model = GridSearchCV(pipeline, grid[algo], cv=10, n_jobs=-1)
    # 4.2 Fit the model
    model.fit(X_train, y_train)
    # 4.3 Save it to the fit models dictionary
    fit_models[algo] = model
    print(algo, 'model has been fit.')

fit_models

# Create predictions from fit model dictionary
fit_models['rf'].predict(X_test)

# Import mean squared error metric
from sklearn.metrics import mean_squared_error

# Create test predictions and calculate MSE
for algo, model in fit_models.items():
     yhat = model.predict(X_test)
     print('MSE for', algo, 'is', mean_squared_error(y_test, yhat))

# Access best estimator from trained Grid Search model
bestmod = fit_models['rf'].best_estimator_

# Create prediction from best estimator model
bestmod.predict(X_test)

# Save model using pickle
import pickle as pkl
with open('saved_model.pkl', 'wb') as f:
     pkl.dump(bestmod, f)

# Delete best model
del bestmod

# Load saved model
with open('saved_model.pkl', 'rb') as f:
     model = pkl.load(f)

# Predict with reloaded model
model.predict(X_test)

